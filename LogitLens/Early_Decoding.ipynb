{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2FudheeHS9H1aKAc5nEkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbaeumel/MI_tutorials/blob/main/Early_Decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the Demo Notebook by Jack Merullo - \"Language Models Implement Simple Word2Vec-Style Vector Arithmetic\""
      ],
      "metadata": {
        "id": "eXflGsXIzajB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does GPT2-medium predict for the sentence 'The capital city of Poland is'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f_hScVle0kKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "WzaEJAVV15t-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "HDj3BjWm17oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Todo: Tokenize input\n",
        "text = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "\n",
        "\n",
        "# Todo: Generate output\n"
      ],
      "metadata": {
        "id": "H9jXG2N5zpTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to understand how GPT2-medium builds the prediction for 'The capital city of France is ' layer by layer."
      ],
      "metadata": {
        "id": "PbMvyFjZ0UJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Let's get a feel for the model strucure"
      ],
      "metadata": {
        "id": "6Z3CVzBW1vs8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c73I49dszTKn",
        "outputId": "761f8bd4-8b10-4f9d-d562-6e72cec8da25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 1024)\n",
            "    (wpe): Embedding(1024, 1024)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x GPT2Block(\n",
            "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
            "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
            "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little bit of magic: output_hidden_states=True"
      ],
      "metadata": {
        "id": "acw_hZvx5VQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "hWeA-oeV5Xgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Te0iZUov6fhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Input\n",
        "text = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "last_token_position = encoded_input[\"input_ids\"].size(1)-1  # Last token index\n",
        "\n",
        "# Todo: Forward pass through the model to capture intermediate predictions\n",
        "\n",
        "# Todo: Extract residual streams (hidden states) after each layer\n",
        "\n",
        "# Todo: Decode top 5 predictions after each layer\n",
        "top_k = 5\n",
        "intermediate_predictions = []\n",
        "for layer_idx, hidden_state in enumerate(hidden_states):\n",
        "    # Take the hidden state at the last token position\n",
        "    last_token_hidden_state = hidden_state[:, last_token_position, :]\n",
        "\n",
        "    # Todo: Get the top-k predictions\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, k=top_k, dim=-1)\n",
        "    top_k_tokens = tokenizer.batch_decode(top_k_indices[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store layer predictions\n",
        "    intermediate_predictions.append({\n",
        "        \"layer\": layer_idx,\n",
        "        \"predictions\": [{\"token\": token, \"probability\": prob.item()} for token, prob in zip(top_k_tokens, top_k_probs[0])]\n",
        "    })"
      ],
      "metadata": {
        "id": "uVJcWWG55yFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty-print\n",
        "for layer_prediction in intermediate_predictions:\n",
        "    layer = layer_prediction[\"layer\"]\n",
        "    print(f\"\\nLayer {layer} Predictions:\")\n",
        "    for prediction in layer_prediction[\"predictions\"]:\n",
        "        token = prediction[\"token\"]\n",
        "        probability = prediction[\"probability\"]\n",
        "        print(f\"  Token: '{token}' | Probability: {probability:.4f}\")\n"
      ],
      "metadata": {
        "id": "e2pemdc47cWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens in layer 19?**\n",
        "\n",
        "How can we find out?\n",
        "\n",
        "Unfortunately there is no built-in function (like output_hidden_states = True ðŸ˜ž)\n",
        "\n",
        "There is a (manual) solution though: **Hooks**"
      ],
      "metadata": {
        "id": "tstxhzlI5wzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an exercise, let's re-implement the layer wise predictions without output_hidden_states = True"
      ],
      "metadata": {
        "id": "ppzQf_VL9K0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model without hidden states\n",
        "model_name = \"gpt2-medium\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "h7zqUbK79YBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "uVIEwzHh_q6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "class GPT2WithHooks:\n",
        "    def __init__(self, model_name=\"gpt2-medium\", top_k=5, device=None):\n",
        "        # Load model and tokenizer\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.TOP_K = top_k\n",
        "\n",
        "        # Set device (default to 'cuda' if available, otherwise 'cpu')\n",
        "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize the activations dictionary\n",
        "        self.set_hooks_gpt2()\n",
        "\n",
        "    def set_hooks_gpt2(self):\n",
        "        final_layer = self.model.config.n_layer - 1\n",
        "\n",
        "        for attr in [\"activations_\"]:\n",
        "            if not hasattr(self.model, attr):\n",
        "                setattr(self.model, attr, {})\n",
        "\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                if \"mlp\" in name or \"attn\" in name:\n",
        "                    if \"attn\" in name:\n",
        "                        num_tokens = list(output[0].size())[1]\n",
        "                        # TODO: Store activations\n",
        "                    elif \"mlp\" in name:\n",
        "                        num_tokens = list(output[0].size())[0]  # [num_tokens, 3072] for values;\n",
        "                        # TODO: Store activations\n",
        "                elif \"residual\" in name:\n",
        "                    num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "                    if name == \"layer_residual_\" + str(final_layer):\n",
        "                        self.model.activations_[name] = self.model.activations_[\"intermediate_residual_\" + str(final_layer)] + self.model.activations_[\"mlp_\" + str(final_layer)]\n",
        "                    else:\n",
        "                        # TODO: Store activations\n",
        "\n",
        "            return hook\n",
        "\n",
        "        # Register hooks\n",
        "        for i in range(self.model.config.n_layer):\n",
        "            if i != 0:\n",
        "                # TODO: register hooks for layer residual\n",
        "            # TODO: register hook for intermediate residual & mlp\n",
        "\n",
        "        # TODO: register hook for last layer residual\n",
        "\n",
        "    def forward(self, text):\n",
        "        encoded_input = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        # Forward pass to trigger hooks\n",
        "        with torch.no_grad():\n",
        "            self.model(**encoded_input)\n",
        "\n",
        "        # Return activations\n",
        "        return self.model.activations_\n",
        "\n",
        "    def get_resid_predictions(self, sentence):\n",
        "        \"\"\"\n",
        "        This function computes the predictions at different layers of GPT-2 using activations from residual layers.\n",
        "        \"\"\"\n",
        "        layer_residual_preds = []\n",
        "\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens.to(self.device)\n",
        "\n",
        "        # Output with hidden states\n",
        "        output = self.model(**tokens, output_hidden_states=True)\n",
        "\n",
        "        for layer in self.model.activations_.keys():\n",
        "            if \"layer_residual\" in layer:\n",
        "                normed = self.model.transformer.ln_f(self.model.activations_[layer])\n",
        "\n",
        "                logits = torch.matmul(self.model.lm_head.weight, normed.T)\n",
        "\n",
        "                probs = F.softmax(logits.T[0], dim=-1)\n",
        "\n",
        "                probs = torch.reshape(probs, (-1,)).detach().cpu().numpy()\n",
        "\n",
        "                assert np.abs(np.sum(probs) - 1) <= 0.01, str(np.abs(np.sum(probs) - 1)) + layer\n",
        "\n",
        "                probs_ = []\n",
        "                for index, prob in enumerate(probs):\n",
        "                    probs_.append((index, prob))\n",
        "\n",
        "                # Get top-k predictions\n",
        "                top_k = sorted(probs_, key=lambda x: x[1], reverse=True)[:self.TOP_K]\n",
        "                top_k = [(t[1].item(), self.tokenizer.decode(t[0])) for t in top_k]\n",
        "\n",
        "            if \"layer_residual\" in layer:\n",
        "                layer_residual_preds.append(top_k)\n",
        "\n",
        "        return layer_residual_preds\n",
        "\n",
        "    def display_predictions(self, sentence):\n",
        "        layer_residual_preds= self.get_resid_predictions(sentence)\n",
        "\n",
        "        print(f\"Predictions for: {sentence}\\n\")\n",
        "\n",
        "        # Display layer residual predictions\n",
        "        print(\"Layer Residual Predictions:\")\n",
        "        for i, preds in enumerate(layer_residual_preds):\n",
        "            print(f\"Layer {i}: {preds}\")\n",
        "\n",
        "# Example usage\n",
        "gpt2_with_hooks = GPT2WithHooks()\n",
        "\n",
        "# Run some text through the model to collect activations\n",
        "sentence = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "gpt2_with_hooks.display_predictions(sentence)\n"
      ],
      "metadata": {
        "id": "YykAYsR2FXl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's register more hooks!**\n",
        "\n",
        "Task: By looking at the model structure, register a hook for each Attention output."
      ],
      "metadata": {
        "id": "YLOckRKGHwYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "class GPT2WithHooks:\n",
        "    def __init__(self, model_name=\"gpt2-medium\", top_k=5, device=None):\n",
        "        # Load model and tokenizer\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.TOP_K = top_k\n",
        "\n",
        "        # Set device (default to 'cuda' if available, otherwise 'cpu')\n",
        "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize the activations dictionary\n",
        "        self.set_hooks_gpt2()\n",
        "\n",
        "    def set_hooks_gpt2(self):\n",
        "        final_layer = self.model.config.n_layer - 1\n",
        "\n",
        "        for attr in [\"activations_\"]:\n",
        "            if not hasattr(self.model, attr):\n",
        "                setattr(self.model, attr, {})\n",
        "\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                if \"mlp\" in name or \"attn\" in name:\n",
        "                    if \"attn\" in name:\n",
        "                        num_tokens = list(output[0].size())[1]\n",
        "                        # TODO: Store activation\n",
        "                    elif \"mlp\" in name:\n",
        "                        num_tokens = list(output[0].size())[0]  # [num_tokens, 3072] for values;\n",
        "                        self.model.activations_[name] = output[0][num_tokens - 1].detach()\n",
        "                elif \"residual\" in name:\n",
        "                    num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "                    if name == \"layer_residual_\" + str(final_layer):\n",
        "                        self.model.activations_[name] = self.model.activations_[\n",
        "                                                            \"intermediate_residual_\" + str(final_layer)] + \\\n",
        "                                                        self.model.activations_[\"mlp_\" + str(final_layer)]\n",
        "                    else:\n",
        "                        self.model.activations_[name] = input[0][:, num_tokens - 1].detach()\n",
        "\n",
        "            return hook\n",
        "\n",
        "        # Register hooks\n",
        "        self.model.transformer.h[0].ln_1.register_forward_hook(get_activation(\"input_embedding\"))\n",
        "\n",
        "        for i in range(self.model.config.n_layer):\n",
        "            if i != 0:\n",
        "                self.model.transformer.h[i].ln_1.register_forward_hook(get_activation(\"layer_residual_\" + str(i - 1)))\n",
        "            self.model.transformer.h[i].ln_2.register_forward_hook(get_activation(\"intermediate_residual_\" + str(i)))\n",
        "\n",
        "            # TODO: Register hooks for the attention outputs\n",
        "            self.model.transformer.h[i].mlp.register_forward_hook(get_activation(\"mlp_\" + str(i)))\n",
        "            self.model.transformer.h[i].mlp.c_proj.register_forward_hook(get_activation(\"m_coef_\" + str(i)))\n",
        "\n",
        "        self.model.transformer.ln_f.register_forward_hook(get_activation(\"layer_residual_\" + str(final_layer)))\n",
        "\n",
        "    def forward(self, text):\n",
        "        encoded_input = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        # Forward pass to trigger hooks\n",
        "        with torch.no_grad():\n",
        "            self.model(**encoded_input)\n",
        "\n",
        "        # Return activations\n",
        "        return self.model.activations_\n",
        "\n",
        "    def get_resid_predictions(self, sentence):\n",
        "        \"\"\"\n",
        "        This function computes the intermediate predictions at different layers of GPT-2\n",
        "        using activations from residual layers and intermediate layers.\n",
        "        \"\"\"\n",
        "        layer_residual_preds = []\n",
        "        intermed_residual_preds = []\n",
        "\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens.to(self.device)\n",
        "\n",
        "        # Output with hidden states\n",
        "        output = self.model(**tokens, output_hidden_states=True)\n",
        "\n",
        "        for layer in self.model.activations_.keys():\n",
        "            if \"layer_residual\" in layer or \"intermediate_residual\" in layer:\n",
        "                normed = self.model.transformer.ln_f(self.model.activations_[layer])\n",
        "\n",
        "                logits = torch.matmul(self.model.lm_head.weight, normed.T)\n",
        "\n",
        "                probs = F.softmax(logits.T[0], dim=-1)\n",
        "\n",
        "                probs = torch.reshape(probs, (-1,)).detach().cpu().numpy()\n",
        "\n",
        "                assert np.abs(np.sum(probs) - 1) <= 0.01, str(np.abs(np.sum(probs) - 1)) + layer\n",
        "\n",
        "                probs_ = []\n",
        "                for index, prob in enumerate(probs):\n",
        "                    probs_.append((index, prob))\n",
        "\n",
        "                # Get top-k predictions\n",
        "                top_k = sorted(probs_, key=lambda x: x[1], reverse=True)[:self.TOP_K]\n",
        "                top_k = [(t[1].item(), self.tokenizer.decode(t[0])) for t in top_k]\n",
        "\n",
        "            if \"layer_residual\" in layer:\n",
        "                layer_residual_preds.append(top_k)\n",
        "            elif \"intermediate_residual\" in layer:\n",
        "                intermed_residual_preds.append(top_k)\n",
        "\n",
        "        return layer_residual_preds, intermed_residual_preds\n",
        "\n",
        "    def display_predictions(self, sentence):\n",
        "        layer_residual_preds, intermed_residual_preds = self.get_resid_predictions(sentence)\n",
        "\n",
        "        print(f\"Predictions for: {sentence}\\n\")\n",
        "\n",
        "        # Display layer residual predictions\n",
        "        print(\"Layer Residual Predictions:\")\n",
        "        for i, preds in enumerate(layer_residual_preds):\n",
        "            print(f\"Layer {i}: {preds}\")\n",
        "\n",
        "        # Display intermediate residual predictions\n",
        "        print(\"\\nIntermediate Residual Predictions:\")\n",
        "        for i, preds in enumerate(intermed_residual_preds):\n",
        "            print(f\"Layer {i}: {preds}\")\n",
        "\n",
        "# Example usage\n",
        "gpt2_with_hooks = GPT2WithHooks()\n",
        "\n",
        "# Run some text through the model to collect activations\n",
        "sentence = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "gpt2_with_hooks.display_predictions(sentence)\n"
      ],
      "metadata": {
        "id": "5aOu-EnZCeQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What exactly happens between Intermediate Residual Predictions and Layer Residual Predictions?\n",
        "\n"
      ],
      "metadata": {
        "id": "SC2RLcJfDynK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
        "import inspect\n",
        "print(inspect.getsource(GPT2Block.forward))"
      ],
      "metadata": {
        "id": "efMj4Xvx3chT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
