{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqcduSPSnvZ2P+KjFDhm0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbaeumel/MI_tutorials/blob/main/Project_Sessions_3_4_5_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do LLMs do addition?\n",
        "\n",
        "This is a mini-project to test your new skills in **early decoding**, **probing**, and **causal intervention**.\n",
        "\n",
        "We have the following question: How do LLMs do addition?\n",
        "\n",
        "For instance, *161 + 224 = 385*"
      ],
      "metadata": {
        "id": "BqEIUKE8v8RZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Prediction Accuracy\n",
        "\n",
        "We start by looking at the performance of *meta-llama/Llama-3.2-1B* on addition tasks."
      ],
      "metadata": {
        "id": "cJe8mm05wzaQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCqk_bPyvoS6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "# Login to Huggingface to get access to model parameters\n",
        "# Paste your token here\n",
        "login('')\n",
        "\n",
        "\n",
        "# Define prompts\n",
        "prompts = [\"122 + 125 = 247; 161 + 224 =\",\n",
        "           \"122 + 125 = 247; 161 + 395 =\",\n",
        "           \"122 + 125 = 247; 353 + 214 =\",\n",
        "           \"122 + 125 = 247; 499 + 412 =\",\n",
        "           \"122 + 125 = 247; 121 + 540 =\"]\n",
        "\n",
        "model_name = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\" # \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Load the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Evaluate the model on each prompt\n",
        "for prompt in prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=inputs['input_ids'].shape[1] + 10)\n",
        "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Model completion: {completion}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 - Early Decoding\n",
        "\n",
        "Try to answer these questions:\n",
        "1.   From which layer on does Llama predict the\n",
        "correct result?\n",
        "2.   What is the role of Attention and MLP layers?\n",
        "\n",
        "To answer 1) you can use `output_hidden_states=True`, normalize the hidden states with the final layer norm (Use `print(model)` to find about names of layers in *LLama-3.2*), and apply the language modeling head of the model to get logits. Then you simply turn the logit into probabilities.\n",
        "\n",
        "**Difficulty of 1): Easy**, since you can copy heavily from the *Early_Decoding.ipynb* notebook.\n",
        "\n",
        "To answer 2) you will have to use hooks. Go and have a look at the forward loop of the model to find the best possible position to hook (https://github.com/huggingface/transformers/blob/v4.49.0/src/transformers/models/llama/modeling_llama.py). Once you figured this out, you can rely heavily on the *Early_Decoding.ipynb* notebook (again, use `print(model)` to find about names of layers in *LLama-3.2*)\n",
        "\n",
        "**Difficulty of 2): More advanced**, since you have to think about the information flow in the model, to choose the best hooks."
      ],
      "metadata": {
        "id": "J-VGQ5Wf0MI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "x459KnboxrYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3 - Causal Interventions\n",
        "\n",
        "Try to answer these questions:\n",
        "\n",
        "1.   What layer (or module in that layer) is responsible for introducing operands into the 'calculation'?  \n",
        "2.   Can we find out anything interesting about the use of operators?\n",
        "\n",
        "Use pyvene to intervene on layers and modules at the last token position.\n",
        "Do your investigations on a single prompt (or at most a handful of prompt).\n",
        "Think carefully about the design of your interventions, so that your results are clearly interpretable."
      ],
      "metadata": {
        "id": "bSjSNJhm8KgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "WymTgThO8LkO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}