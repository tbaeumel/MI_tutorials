{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIyd5U4jU0BzP7lIUYs8Bx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dcf32b0923a74813abd5192ae5a14807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad0f89fd61d04600b10909c8a7e41846",
              "IPY_MODEL_86ee5288a09e4dc4b0a9c86c117f4242",
              "IPY_MODEL_96e50273772b4bac9ef44638d01a9869"
            ],
            "layout": "IPY_MODEL_fc19bae2923e4a3da912eec173f5b9fa"
          }
        },
        "ad0f89fd61d04600b10909c8a7e41846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adf67b702272484db0236820b0d44bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_6dc82738e5154d8da253026cc0a12149",
            "value": "generation_config.json: 100%"
          }
        },
        "86ee5288a09e4dc4b0a9c86c117f4242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b71d6d7637094394a8844fd673147844",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d096ea16d5994ac9b1fca131f6ec127e",
            "value": 124
          }
        },
        "96e50273772b4bac9ef44638d01a9869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8244ef7316874272ae4313a637143ee9",
            "placeholder": "​",
            "style": "IPY_MODEL_72bec83f38e24c4b896feb51aaebc2e4",
            "value": " 124/124 [00:00&lt;00:00, 6.82kB/s]"
          }
        },
        "fc19bae2923e4a3da912eec173f5b9fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf67b702272484db0236820b0d44bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc82738e5154d8da253026cc0a12149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b71d6d7637094394a8844fd673147844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d096ea16d5994ac9b1fca131f6ec127e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8244ef7316874272ae4313a637143ee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72bec83f38e24c4b896feb51aaebc2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbaeumel/MI_tutorials/blob/main/Early_Decoding_By_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the Demo Notebook by Jack Merullo - \"Language Models Implement Simple Word2Vec-Style Vector Arithmetic\""
      ],
      "metadata": {
        "id": "eXflGsXIzajB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does GPT2-medium predict for the sentence 'The capital city of Poland is'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f_hScVle0kKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "WzaEJAVV15t-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dcf32b0923a74813abd5192ae5a14807",
            "ad0f89fd61d04600b10909c8a7e41846",
            "86ee5288a09e4dc4b0a9c86c117f4242",
            "96e50273772b4bac9ef44638d01a9869",
            "fc19bae2923e4a3da912eec173f5b9fa",
            "adf67b702272484db0236820b0d44bd2",
            "6dc82738e5154d8da253026cc0a12149",
            "b71d6d7637094394a8844fd673147844",
            "d096ea16d5994ac9b1fca131f6ec127e",
            "8244ef7316874272ae4313a637143ee9",
            "72bec83f38e24c4b896feb51aaebc2e4"
          ]
        },
        "id": "HDj3BjWm17oE",
        "outputId": "72b84925-d9ea-44e6-8ee2-c2ddd263f6c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcf32b0923a74813abd5192ae5a14807"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input\n",
        "text = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Generate output\n",
        "output_ids = model.generate(encoded_input['input_ids'], max_new_tokens=2, num_return_sequences=1)\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9jXG2N5zpTB",
        "outputId": "ae982e77-3696-44d6-adc4-a0286be41ff0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the capital of France?\n",
            "A: Paris\n",
            "Q: What is the capital of Poland?\n",
            "A: Warsaw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to understand how GPT2-medium builds the prediction for 'The capital city of France is ' layer by layer."
      ],
      "metadata": {
        "id": "PbMvyFjZ0UJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Let's get a feel for the model strucure"
      ],
      "metadata": {
        "id": "6Z3CVzBW1vs8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c73I49dszTKn",
        "outputId": "761f8bd4-8b10-4f9d-d562-6e72cec8da25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 1024)\n",
            "    (wpe): Embedding(1024, 1024)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x GPT2Block(\n",
            "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
            "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
            "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little bit of magic: output_hidden_states=True"
      ],
      "metadata": {
        "id": "acw_hZvx5VQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWeA-oeV5Xgv",
        "outputId": "77c3c9eb-767a-4960-ffe6-7a77730de928"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Te0iZUov6fhB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Input\n",
        "text = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Forward pass through the model to capture intermediate predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded_input)\n",
        "\n",
        "# Extract residual streams (hidden states) after each layer\n",
        "hidden_states = outputs.hidden_states\n",
        "last_token_position = encoded_input[\"input_ids\"].size(1)-1  # Last token index\n",
        "\n",
        "# Decode top 5 predictions after each layer\n",
        "top_k = 5\n",
        "intermediate_predictions = []\n",
        "for layer_idx, hidden_state in enumerate(hidden_states):\n",
        "    # Take the hidden state at the last token position\n",
        "    last_token_hidden_state = hidden_state[:, last_token_position, :]\n",
        "\n",
        "    # Pass it through the final layer norm and generate logits\n",
        "    normalized_hidden_state = model.transformer.ln_f(last_token_hidden_state)\n",
        "    logits = model.lm_head(normalized_hidden_state)\n",
        "\n",
        "    # Calculate probabilities using softmax\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top-k predictions\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, k=top_k, dim=-1)\n",
        "    top_k_tokens = tokenizer.batch_decode(top_k_indices[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store layer predictions\n",
        "    intermediate_predictions.append({\n",
        "        \"layer\": layer_idx,\n",
        "        \"predictions\": [{\"token\": token, \"probability\": prob.item()} for token, prob in zip(top_k_tokens, top_k_probs[0])]\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVJcWWG55yFv",
        "outputId": "5cad2e7d-0654-492d-a9cf-54e8b8225429"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'layer': 0, 'predictions': [{'token': ' unden', 'probability': 0.9470192193984985}, {'token': ' nodd', 'probability': 0.021967994049191475}, {'token': ' enthusi', 'probability': 0.011501138098537922}, {'token': ':', 'probability': 0.0050586313009262085}, {'token': ' neighb', 'probability': 0.005044832825660706}]}, {'layer': 1, 'predictions': [{'token': ' (', 'probability': 0.02316311188042164}, {'token': ' [', 'probability': 0.018392831087112427}, {'token': ' The', 'probability': 0.012803658843040466}, {'token': ':', 'probability': 0.009204289875924587}, {'token': ',', 'probability': 0.00885365903377533}]}, {'layer': 2, 'predictions': [{'token': ' A', 'probability': 0.019568055868148804}, {'token': ' The', 'probability': 0.01659923605620861}, {'token': ' (', 'probability': 0.014010699465870857}, {'token': ' [', 'probability': 0.010149898007512093}, {'token': ' Is', 'probability': 0.0076424493454396725}]}, {'layer': 3, 'predictions': [{'token': ' A', 'probability': 0.026887256652116776}, {'token': ' [', 'probability': 0.018573077395558357}, {'token': ' (', 'probability': 0.017433302477002144}, {'token': ' The', 'probability': 0.013812455348670483}, {'token': ' At', 'probability': 0.011615313589572906}]}, {'layer': 4, 'predictions': [{'token': ' A', 'probability': 0.03929822891950607}, {'token': ' [', 'probability': 0.022901298478245735}, {'token': ' (', 'probability': 0.020305311307311058}, {'token': ' Act', 'probability': 0.018195508047938347}, {'token': ' At', 'probability': 0.01783018745481968}]}, {'layer': 5, 'predictions': [{'token': ' A', 'probability': 0.03888517618179321}, {'token': ' [', 'probability': 0.0246482715010643}, {'token': ' At', 'probability': 0.023253263905644417}, {'token': ' Q', 'probability': 0.014608464203774929}, {'token': ' (', 'probability': 0.011924294754862785}]}, {'layer': 6, 'predictions': [{'token': ' A', 'probability': 0.03538786247372627}, {'token': ' M', 'probability': 0.03339677304029465}, {'token': ' No', 'probability': 0.029982512816786766}, {'token': ' At', 'probability': 0.01403931062668562}, {'token': ' The', 'probability': 0.010575392283499241}]}, {'layer': 7, 'predictions': [{'token': ' No', 'probability': 0.09016582369804382}, {'token': ' M', 'probability': 0.0870906338095665}, {'token': ' A', 'probability': 0.04744257032871246}, {'token': ' The', 'probability': 0.026001401245594025}, {'token': ' C', 'probability': 0.023228175938129425}]}, {'layer': 8, 'predictions': [{'token': ' C', 'probability': 0.10190927982330322}, {'token': ' A', 'probability': 0.0560058131814003}, {'token': ' No', 'probability': 0.04421882703900337}, {'token': ' The', 'probability': 0.03406989946961403}, {'token': ' M', 'probability': 0.024924559518694878}]}, {'layer': 9, 'predictions': [{'token': ' A', 'probability': 0.0641852542757988}, {'token': ' The', 'probability': 0.05002090707421303}, {'token': ' C', 'probability': 0.04732096567749977}, {'token': ' P', 'probability': 0.04097893834114075}, {'token': ' H', 'probability': 0.03743017837405205}]}, {'layer': 10, 'predictions': [{'token': ' A', 'probability': 0.04627809673547745}, {'token': ' C', 'probability': 0.04145506024360657}, {'token': ' No', 'probability': 0.03463997319340706}, {'token': ' nil', 'probability': 0.030710455030202866}, {'token': ' The', 'probability': 0.02961716242134571}]}, {'layer': 11, 'predictions': [{'token': ' A', 'probability': 0.06454548984766006}, {'token': ' The', 'probability': 0.03523924574255943}, {'token': ' G', 'probability': 0.03315829858183861}, {'token': ' C', 'probability': 0.03144809603691101}, {'token': ' N', 'probability': 0.024261722341179848}]}, {'layer': 12, 'predictions': [{'token': ' A', 'probability': 0.06306032091379166}, {'token': ' C', 'probability': 0.05588363856077194}, {'token': ' N', 'probability': 0.052276697009801865}, {'token': ' None', 'probability': 0.045362163335084915}, {'token': ' P', 'probability': 0.04448758065700531}]}, {'layer': 13, 'predictions': [{'token': ' Unknown', 'probability': 0.08687000721693039}, {'token': ' None', 'probability': 0.08334797620773315}, {'token': ' C', 'probability': 0.07776157557964325}, {'token': ' G', 'probability': 0.05151400342583656}, {'token': ' A', 'probability': 0.051126088947057724}]}, {'layer': 14, 'predictions': [{'token': ' C', 'probability': 0.17199458181858063}, {'token': ' P', 'probability': 0.08765175938606262}, {'token': ' N', 'probability': 0.08286633342504501}, {'token': ' G', 'probability': 0.06292077153921127}, {'token': ' B', 'probability': 0.044694509357213974}]}, {'layer': 15, 'predictions': [{'token': ' St', 'probability': 0.1674150675535202}, {'token': ' N', 'probability': 0.09440936893224716}, {'token': ' G', 'probability': 0.08308736979961395}, {'token': ' P', 'probability': 0.08226532489061356}, {'token': ' Poland', 'probability': 0.07120455056428909}]}, {'layer': 16, 'predictions': [{'token': ' Poland', 'probability': 0.878180742263794}, {'token': ' P', 'probability': 0.029262306168675423}, {'token': ' St', 'probability': 0.016060682013630867}, {'token': ' Pol', 'probability': 0.01333635300397873}, {'token': ' Warsaw', 'probability': 0.006081582047045231}]}, {'layer': 17, 'predictions': [{'token': ' Poland', 'probability': 0.9498633742332458}, {'token': ' Warsaw', 'probability': 0.04541599005460739}, {'token': ' Polish', 'probability': 0.004187237937003374}, {'token': ' Poles', 'probability': 0.00020395955652929842}, {'token': ' Budapest', 'probability': 0.00012969874660484493}]}, {'layer': 18, 'predictions': [{'token': ' Poland', 'probability': 0.91972416639328}, {'token': ' Warsaw', 'probability': 0.0732327401638031}, {'token': ' Polish', 'probability': 0.006632524076849222}, {'token': ' Poles', 'probability': 0.000153074724948965}, {'token': ' Budapest', 'probability': 9.34480267460458e-05}]}, {'layer': 19, 'predictions': [{'token': ' Poland', 'probability': 0.7380667328834534}, {'token': ' Warsaw', 'probability': 0.2567860782146454}, {'token': ' Polish', 'probability': 0.004197099711745977}, {'token': ' Prague', 'probability': 0.0004322587337810546}, {'token': ' Budapest', 'probability': 0.000329066562699154}]}, {'layer': 20, 'predictions': [{'token': ' Warsaw', 'probability': 0.8590309619903564}, {'token': ' Poland', 'probability': 0.13808636367321014}, {'token': ' Polish', 'probability': 0.0018533499678596854}, {'token': ' Budapest', 'probability': 0.0005096513777971268}, {'token': ' Prague', 'probability': 0.00037876167334616184}]}, {'layer': 21, 'predictions': [{'token': ' Warsaw', 'probability': 0.9741463661193848}, {'token': ' Poland', 'probability': 0.022860031574964523}, {'token': ' Prague', 'probability': 0.0014567093458026648}, {'token': ' Budapest', 'probability': 0.000764022464863956}, {'token': ' Polish', 'probability': 0.000555414822883904}]}, {'layer': 22, 'predictions': [{'token': ' Warsaw', 'probability': 0.9729665517807007}, {'token': ' Poland', 'probability': 0.02499479427933693}, {'token': ' Polish', 'probability': 0.0009886075276881456}, {'token': ' Prague', 'probability': 0.0008147604530677199}, {'token': ' Budapest', 'probability': 0.00011722456838469952}]}, {'layer': 23, 'predictions': [{'token': ' Warsaw', 'probability': 0.9295447468757629}, {'token': ' Poland', 'probability': 0.011738079600036144}, {'token': ' Prague', 'probability': 0.009622976183891296}, {'token': ' Budapest', 'probability': 0.004818271845579147}, {'token': ' K', 'probability': 0.002819966757670045}]}, {'layer': 24, 'predictions': [{'token': '\\n', 'probability': 0.0041842395439744}, {'token': ' P', 'probability': 0.003164894413203001}, {'token': ' W', 'probability': 0.0030859708786010742}, {'token': ' The', 'probability': 0.003084323136135936}, {'token': ' I', 'probability': 0.0027803380507975817}]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty-print\n",
        "for layer_prediction in intermediate_predictions:\n",
        "    layer = layer_prediction[\"layer\"]\n",
        "    print(f\"\\nLayer {layer} Predictions:\")\n",
        "    for prediction in layer_prediction[\"predictions\"]:\n",
        "        token = prediction[\"token\"]\n",
        "        probability = prediction[\"probability\"]\n",
        "        print(f\"  Token: '{token}' | Probability: {probability:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pemdc47cWW",
        "outputId": "57d3ce46-b561-4702-fa38-2dbfb4c17b89"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 0 Predictions:\n",
            "  Token: ' unden' | Probability: 0.9470\n",
            "  Token: ' nodd' | Probability: 0.0220\n",
            "  Token: ' enthusi' | Probability: 0.0115\n",
            "  Token: ':' | Probability: 0.0051\n",
            "  Token: ' neighb' | Probability: 0.0050\n",
            "\n",
            "Layer 1 Predictions:\n",
            "  Token: ' (' | Probability: 0.0232\n",
            "  Token: ' [' | Probability: 0.0184\n",
            "  Token: ' The' | Probability: 0.0128\n",
            "  Token: ':' | Probability: 0.0092\n",
            "  Token: ',' | Probability: 0.0089\n",
            "\n",
            "Layer 2 Predictions:\n",
            "  Token: ' A' | Probability: 0.0196\n",
            "  Token: ' The' | Probability: 0.0166\n",
            "  Token: ' (' | Probability: 0.0140\n",
            "  Token: ' [' | Probability: 0.0101\n",
            "  Token: ' Is' | Probability: 0.0076\n",
            "\n",
            "Layer 3 Predictions:\n",
            "  Token: ' A' | Probability: 0.0269\n",
            "  Token: ' [' | Probability: 0.0186\n",
            "  Token: ' (' | Probability: 0.0174\n",
            "  Token: ' The' | Probability: 0.0138\n",
            "  Token: ' At' | Probability: 0.0116\n",
            "\n",
            "Layer 4 Predictions:\n",
            "  Token: ' A' | Probability: 0.0393\n",
            "  Token: ' [' | Probability: 0.0229\n",
            "  Token: ' (' | Probability: 0.0203\n",
            "  Token: ' Act' | Probability: 0.0182\n",
            "  Token: ' At' | Probability: 0.0178\n",
            "\n",
            "Layer 5 Predictions:\n",
            "  Token: ' A' | Probability: 0.0389\n",
            "  Token: ' [' | Probability: 0.0246\n",
            "  Token: ' At' | Probability: 0.0233\n",
            "  Token: ' Q' | Probability: 0.0146\n",
            "  Token: ' (' | Probability: 0.0119\n",
            "\n",
            "Layer 6 Predictions:\n",
            "  Token: ' A' | Probability: 0.0354\n",
            "  Token: ' M' | Probability: 0.0334\n",
            "  Token: ' No' | Probability: 0.0300\n",
            "  Token: ' At' | Probability: 0.0140\n",
            "  Token: ' The' | Probability: 0.0106\n",
            "\n",
            "Layer 7 Predictions:\n",
            "  Token: ' No' | Probability: 0.0902\n",
            "  Token: ' M' | Probability: 0.0871\n",
            "  Token: ' A' | Probability: 0.0474\n",
            "  Token: ' The' | Probability: 0.0260\n",
            "  Token: ' C' | Probability: 0.0232\n",
            "\n",
            "Layer 8 Predictions:\n",
            "  Token: ' C' | Probability: 0.1019\n",
            "  Token: ' A' | Probability: 0.0560\n",
            "  Token: ' No' | Probability: 0.0442\n",
            "  Token: ' The' | Probability: 0.0341\n",
            "  Token: ' M' | Probability: 0.0249\n",
            "\n",
            "Layer 9 Predictions:\n",
            "  Token: ' A' | Probability: 0.0642\n",
            "  Token: ' The' | Probability: 0.0500\n",
            "  Token: ' C' | Probability: 0.0473\n",
            "  Token: ' P' | Probability: 0.0410\n",
            "  Token: ' H' | Probability: 0.0374\n",
            "\n",
            "Layer 10 Predictions:\n",
            "  Token: ' A' | Probability: 0.0463\n",
            "  Token: ' C' | Probability: 0.0415\n",
            "  Token: ' No' | Probability: 0.0346\n",
            "  Token: ' nil' | Probability: 0.0307\n",
            "  Token: ' The' | Probability: 0.0296\n",
            "\n",
            "Layer 11 Predictions:\n",
            "  Token: ' A' | Probability: 0.0645\n",
            "  Token: ' The' | Probability: 0.0352\n",
            "  Token: ' G' | Probability: 0.0332\n",
            "  Token: ' C' | Probability: 0.0314\n",
            "  Token: ' N' | Probability: 0.0243\n",
            "\n",
            "Layer 12 Predictions:\n",
            "  Token: ' A' | Probability: 0.0631\n",
            "  Token: ' C' | Probability: 0.0559\n",
            "  Token: ' N' | Probability: 0.0523\n",
            "  Token: ' None' | Probability: 0.0454\n",
            "  Token: ' P' | Probability: 0.0445\n",
            "\n",
            "Layer 13 Predictions:\n",
            "  Token: ' Unknown' | Probability: 0.0869\n",
            "  Token: ' None' | Probability: 0.0833\n",
            "  Token: ' C' | Probability: 0.0778\n",
            "  Token: ' G' | Probability: 0.0515\n",
            "  Token: ' A' | Probability: 0.0511\n",
            "\n",
            "Layer 14 Predictions:\n",
            "  Token: ' C' | Probability: 0.1720\n",
            "  Token: ' P' | Probability: 0.0877\n",
            "  Token: ' N' | Probability: 0.0829\n",
            "  Token: ' G' | Probability: 0.0629\n",
            "  Token: ' B' | Probability: 0.0447\n",
            "\n",
            "Layer 15 Predictions:\n",
            "  Token: ' St' | Probability: 0.1674\n",
            "  Token: ' N' | Probability: 0.0944\n",
            "  Token: ' G' | Probability: 0.0831\n",
            "  Token: ' P' | Probability: 0.0823\n",
            "  Token: ' Poland' | Probability: 0.0712\n",
            "\n",
            "Layer 16 Predictions:\n",
            "  Token: ' Poland' | Probability: 0.8782\n",
            "  Token: ' P' | Probability: 0.0293\n",
            "  Token: ' St' | Probability: 0.0161\n",
            "  Token: ' Pol' | Probability: 0.0133\n",
            "  Token: ' Warsaw' | Probability: 0.0061\n",
            "\n",
            "Layer 17 Predictions:\n",
            "  Token: ' Poland' | Probability: 0.9499\n",
            "  Token: ' Warsaw' | Probability: 0.0454\n",
            "  Token: ' Polish' | Probability: 0.0042\n",
            "  Token: ' Poles' | Probability: 0.0002\n",
            "  Token: ' Budapest' | Probability: 0.0001\n",
            "\n",
            "Layer 18 Predictions:\n",
            "  Token: ' Poland' | Probability: 0.9197\n",
            "  Token: ' Warsaw' | Probability: 0.0732\n",
            "  Token: ' Polish' | Probability: 0.0066\n",
            "  Token: ' Poles' | Probability: 0.0002\n",
            "  Token: ' Budapest' | Probability: 0.0001\n",
            "\n",
            "Layer 19 Predictions:\n",
            "  Token: ' Poland' | Probability: 0.7381\n",
            "  Token: ' Warsaw' | Probability: 0.2568\n",
            "  Token: ' Polish' | Probability: 0.0042\n",
            "  Token: ' Prague' | Probability: 0.0004\n",
            "  Token: ' Budapest' | Probability: 0.0003\n",
            "\n",
            "Layer 20 Predictions:\n",
            "  Token: ' Warsaw' | Probability: 0.8590\n",
            "  Token: ' Poland' | Probability: 0.1381\n",
            "  Token: ' Polish' | Probability: 0.0019\n",
            "  Token: ' Budapest' | Probability: 0.0005\n",
            "  Token: ' Prague' | Probability: 0.0004\n",
            "\n",
            "Layer 21 Predictions:\n",
            "  Token: ' Warsaw' | Probability: 0.9741\n",
            "  Token: ' Poland' | Probability: 0.0229\n",
            "  Token: ' Prague' | Probability: 0.0015\n",
            "  Token: ' Budapest' | Probability: 0.0008\n",
            "  Token: ' Polish' | Probability: 0.0006\n",
            "\n",
            "Layer 22 Predictions:\n",
            "  Token: ' Warsaw' | Probability: 0.9730\n",
            "  Token: ' Poland' | Probability: 0.0250\n",
            "  Token: ' Polish' | Probability: 0.0010\n",
            "  Token: ' Prague' | Probability: 0.0008\n",
            "  Token: ' Budapest' | Probability: 0.0001\n",
            "\n",
            "Layer 23 Predictions:\n",
            "  Token: ' Warsaw' | Probability: 0.9295\n",
            "  Token: ' Poland' | Probability: 0.0117\n",
            "  Token: ' Prague' | Probability: 0.0096\n",
            "  Token: ' Budapest' | Probability: 0.0048\n",
            "  Token: ' K' | Probability: 0.0028\n",
            "\n",
            "Layer 24 Predictions:\n",
            "  Token: '\n",
            "' | Probability: 0.0042\n",
            "  Token: ' P' | Probability: 0.0032\n",
            "  Token: ' W' | Probability: 0.0031\n",
            "  Token: ' The' | Probability: 0.0031\n",
            "  Token: ' I' | Probability: 0.0028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens in layer 19?**\n",
        "\n",
        "How can we find out?\n",
        "\n",
        "Unfortunately there is no built-in function (like output_hidden_states = True 😞)\n",
        "\n",
        "There is a (manual) solution though: **Hooks**"
      ],
      "metadata": {
        "id": "tstxhzlI5wzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an exercise, let's re-implement the layer wise predictions without output_hidden_states = True"
      ],
      "metadata": {
        "id": "ppzQf_VL9K0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model without hidden states\n",
        "model_name = \"gpt2-medium\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "h7zqUbK79YBs"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "uVIEwzHh_q6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "class GPT2WithHooks:\n",
        "    def __init__(self, model_name=\"gpt2-medium\", top_k=5, device=None):\n",
        "        # Load model and tokenizer\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.TOP_K = top_k\n",
        "\n",
        "        # Set device (default to 'cuda' if available, otherwise 'cpu')\n",
        "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize the activations dictionary\n",
        "        self.set_hooks_gpt2()\n",
        "\n",
        "    def set_hooks_gpt2(self):\n",
        "        final_layer = self.model.config.n_layer - 1\n",
        "\n",
        "        for attr in [\"activations_\"]:\n",
        "            if not hasattr(self.model, attr):\n",
        "                setattr(self.model, attr, {})\n",
        "\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                if \"mlp\" in name or \"attn\" in name:\n",
        "                    if \"attn\" in name:\n",
        "                        num_tokens = list(output[0].size())[1]\n",
        "                        self.model.activations_[name] = output[0][:, num_tokens - 1].detach()\n",
        "                    elif \"mlp\" in name:\n",
        "                        num_tokens = list(output[0].size())[0]  # [num_tokens, 3072] for values;\n",
        "                        self.model.activations_[name] = output[0][num_tokens - 1].detach()\n",
        "                elif \"residual\" in name:\n",
        "                    num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "                    if name == \"layer_residual_\" + str(final_layer):\n",
        "                        self.model.activations_[name] = self.model.activations_[\"intermediate_residual_\" + str(final_layer)] + self.model.activations_[\"mlp_\" + str(final_layer)]\n",
        "                    else:\n",
        "                        self.model.activations_[name] = input[0][:, num_tokens - 1].detach()\n",
        "\n",
        "            return hook\n",
        "\n",
        "        # Register hooks\n",
        "        for i in range(self.model.config.n_layer):\n",
        "            if i != 0:\n",
        "                self.model.transformer.h[i].ln_1.register_forward_hook(get_activation(\"layer_residual_\" + str(i - 1)))\n",
        "            self.model.transformer.h[i].ln_2.register_forward_hook(get_activation(\"intermediate_residual_\" + str(i)))\n",
        "\n",
        "            self.model.transformer.h[i].attn.register_forward_hook(get_activation(\"attn_\" + str(i)))\n",
        "            self.model.transformer.h[i].mlp.register_forward_hook(get_activation(\"mlp_\" + str(i)))\n",
        "\n",
        "        self.model.transformer.ln_f.register_forward_hook(get_activation(\"layer_residual_\" + str(final_layer)))\n",
        "\n",
        "    def forward(self, text):\n",
        "        encoded_input = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        # Forward pass to trigger hooks\n",
        "        with torch.no_grad():\n",
        "            self.model(**encoded_input)\n",
        "\n",
        "        # Return activations\n",
        "        return self.model.activations_\n",
        "\n",
        "    def get_resid_predictions(self, sentence):\n",
        "        \"\"\"\n",
        "        This function computes the predictions at different layers of GPT-2 using activations from residual layers.\n",
        "        \"\"\"\n",
        "        layer_residual_preds = []\n",
        "\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens.to(self.device)\n",
        "\n",
        "        # Output with hidden states\n",
        "        output = self.model(**tokens, output_hidden_states=True)\n",
        "\n",
        "        for layer in self.model.activations_.keys():\n",
        "            if \"layer_residual\" in layer:\n",
        "                normed = self.model.transformer.ln_f(self.model.activations_[layer])\n",
        "\n",
        "                logits = torch.matmul(self.model.lm_head.weight, normed.T)\n",
        "\n",
        "                probs = F.softmax(logits.T[0], dim=-1)\n",
        "\n",
        "                probs = torch.reshape(probs, (-1,)).detach().cpu().numpy()\n",
        "\n",
        "                assert np.abs(np.sum(probs) - 1) <= 0.01, str(np.abs(np.sum(probs) - 1)) + layer\n",
        "\n",
        "                probs_ = []\n",
        "                for index, prob in enumerate(probs):\n",
        "                    probs_.append((index, prob))\n",
        "\n",
        "                # Get top-k predictions\n",
        "                top_k = sorted(probs_, key=lambda x: x[1], reverse=True)[:self.TOP_K]\n",
        "                top_k = [(t[1].item(), self.tokenizer.decode(t[0])) for t in top_k]\n",
        "\n",
        "            if \"layer_residual\" in layer:\n",
        "                layer_residual_preds.append(top_k)\n",
        "\n",
        "        return layer_residual_preds\n",
        "\n",
        "    def display_predictions(self, sentence):\n",
        "        layer_residual_preds= self.get_resid_predictions(sentence)\n",
        "\n",
        "        print(f\"Predictions for: {sentence}\\n\")\n",
        "\n",
        "        # Display layer residual predictions\n",
        "        print(\"Layer Residual Predictions:\")\n",
        "        for i, preds in enumerate(layer_residual_preds):\n",
        "            print(f\"Layer {i}: {preds}\")\n",
        "\n",
        "# Example usage\n",
        "gpt2_with_hooks = GPT2WithHooks()\n",
        "\n",
        "# Run some text through the model to collect activations\n",
        "sentence = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "gpt2_with_hooks.display_predictions(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YykAYsR2FXl2",
        "outputId": "9c42868c-2d16-47bd-c233-0a94527347db"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for: Q: What is the capital of France?\n",
            "A: Paris\n",
            "Q: What is the capital of Poland?\n",
            "A:\n",
            "\n",
            "Layer Residual Predictions:\n",
            "Layer 0: [(0.02316313423216343, ' ('), (0.018392857164144516, ' ['), (0.012803674675524235, ' The'), (0.00920428428798914, ':'), (0.008853655308485031, ',')]\n",
            "Layer 1: [(0.019568050280213356, ' A'), (0.01659923605620861, ' The'), (0.014010702259838581, ' ('), (0.010149895213544369, ' ['), (0.007642451208084822, ' Is')]\n",
            "Layer 2: [(0.026887251064181328, ' A'), (0.018573066219687462, ' ['), (0.017433306202292442, ' ('), (0.013812464661896229, ' The'), (0.011615300551056862, ' At')]\n",
            "Layer 3: [(0.03929824382066727, ' A'), (0.022901220247149467, ' ['), (0.020305326208472252, ' ('), (0.018195513635873795, ' Act'), (0.017830180004239082, ' At')]\n",
            "Layer 4: [(0.038885049521923065, ' A'), (0.024648193269968033, ' ['), (0.023253243416547775, ' At'), (0.014608520083129406, ' Q'), (0.011924289166927338, ' (')]\n",
            "Layer 5: [(0.035387828946113586, ' A'), (0.033396828919649124, ' M'), (0.029982641339302063, ' No'), (0.014039270579814911, ' At'), (0.010575378313660622, ' The')]\n",
            "Layer 6: [(0.0901661291718483, ' No'), (0.08709050714969635, ' M'), (0.04744250327348709, ' A'), (0.02600138820707798, ' The'), (0.023228120058774948, ' C')]\n",
            "Layer 7: [(0.10190896689891815, ' C'), (0.056005530059337616, ' A'), (0.04421888291835785, ' No'), (0.03406987339258194, ' The'), (0.0249245073646307, ' M')]\n",
            "Layer 8: [(0.06418519467115402, ' A'), (0.050020862370729446, ' The'), (0.04732087627053261, ' C'), (0.04097878187894821, ' P'), (0.03743000328540802, ' H')]\n",
            "Layer 9: [(0.046277809888124466, ' A'), (0.041454847902059555, ' C'), (0.034639887511730194, ' No'), (0.030710438266396523, ' nil'), (0.029617032036185265, ' The')]\n",
            "Layer 10: [(0.06454506516456604, ' A'), (0.035239167511463165, ' The'), (0.033158134669065475, ' G'), (0.0314478725194931, ' C'), (0.024261601269245148, ' N')]\n",
            "Layer 11: [(0.0630597472190857, ' A'), (0.05588334798812866, ' C'), (0.05227632448077202, ' N'), (0.045361969619989395, ' None'), (0.04448726028203964, ' P')]\n",
            "Layer 12: [(0.08686946332454681, ' Unknown'), (0.08334776759147644, ' None'), (0.07776079326868057, ' C'), (0.05151363089680672, ' G'), (0.051125865429639816, ' A')]\n",
            "Layer 13: [(0.17199388146400452, ' C'), (0.08765115588903427, ' P'), (0.08286543935537338, ' N'), (0.0629202201962471, ' G'), (0.04469427838921547, ' B')]\n",
            "Layer 14: [(0.16741488873958588, ' St'), (0.0944088026881218, ' N'), (0.08308672904968262, ' G'), (0.08226499706506729, ' P'), (0.07120406627655029, ' Poland')]\n",
            "Layer 15: [(0.8781789541244507, ' Poland'), (0.02926219440996647, ' P'), (0.01606065034866333, ' St'), (0.01333632506430149, ' Pol'), (0.0060815694741904736, ' Warsaw')]\n",
            "Layer 16: [(0.9498633742332458, ' Poland'), (0.04541594535112381, ' Warsaw'), (0.004187267739325762, ' Polish'), (0.00020395935280248523, ' Poles'), (0.00012969886302016675, ' Budapest')]\n",
            "Layer 17: [(0.9197242259979248, ' Poland'), (0.07323256880044937, ' Warsaw'), (0.006632565520703793, ' Polish'), (0.00015307457942981273, ' Poles'), (9.344794671051204e-05, ' Budapest')]\n",
            "Layer 18: [(0.7380682826042175, ' Poland'), (0.2567846477031708, ' Warsaw'), (0.004197096452116966, ' Polish'), (0.0004322550958022475, ' Prague'), (0.0003290656895842403, ' Budapest')]\n",
            "Layer 19: [(0.8590308427810669, ' Warsaw'), (0.1380864679813385, ' Poland'), (0.0018533550901338458, ' Polish'), (0.0005096498061902821, ' Budapest'), (0.0003787627210840583, ' Prague')]\n",
            "Layer 20: [(0.9741461277008057, ' Warsaw'), (0.022860178723931313, ' Poland'), (0.001456715865060687, ' Prague'), (0.0007640245603397489, ' Budapest'), (0.0005554183735512197, ' Polish')]\n",
            "Layer 21: [(0.972966194152832, ' Warsaw'), (0.024995069950819016, ' Poland'), (0.0009886166080832481, ' Polish'), (0.0008147679618559778, ' Prague'), (0.00011722519411705434, ' Budapest')]\n",
            "Layer 22: [(0.9295430183410645, ' Warsaw'), (0.011738058179616928, ' Poland'), (0.009622957557439804, ' Prague'), (0.0048182448372244835, ' Budapest'), (0.0028199509251862764, ' K')]\n",
            "Layer 23: [(0.6972269415855408, ' Warsaw'), (0.0385914146900177, ' W'), (0.02709244191646576, ' K'), (0.014838638715445995, ' Br'), (0.014428702183067799, ' Po')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's register more hooks!**"
      ],
      "metadata": {
        "id": "YLOckRKGHwYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "class GPT2WithHooks:\n",
        "    def __init__(self, model_name=\"gpt2-medium\", top_k=5, device=None):\n",
        "        # Load model and tokenizer\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.TOP_K = top_k\n",
        "\n",
        "        # Set device (default to 'cuda' if available, otherwise 'cpu')\n",
        "        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize the activations dictionary\n",
        "        self.set_hooks_gpt2()\n",
        "\n",
        "    def set_hooks_gpt2(self):\n",
        "        final_layer = self.model.config.n_layer - 1\n",
        "\n",
        "        for attr in [\"activations_\"]:\n",
        "            if not hasattr(self.model, attr):\n",
        "                setattr(self.model, attr, {})\n",
        "\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                if \"mlp\" in name or \"attn\" in name or \"m_coef\" in name:\n",
        "                    if \"attn\" in name:\n",
        "                        num_tokens = list(output[0].size())[1]\n",
        "                        self.model.activations_[name] = output[0][:, num_tokens - 1].detach()\n",
        "                    elif \"mlp\" in name:\n",
        "                        num_tokens = list(output[0].size())[0]  # [num_tokens, 3072] for values;\n",
        "                        self.model.activations_[name] = output[0][num_tokens - 1].detach()\n",
        "                    elif \"m_coef\" in name:\n",
        "                        num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "                        self.model.activations_[name] = input[0][:, num_tokens - 1].detach()\n",
        "                elif \"residual\" in name or \"embedding\" in name:\n",
        "                    num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "                    if name == \"layer_residual_\" + str(final_layer):\n",
        "                        self.model.activations_[name] = self.model.activations_[\n",
        "                                                            \"intermediate_residual_\" + str(final_layer)] + \\\n",
        "                                                        self.model.activations_[\"mlp_\" + str(final_layer)]\n",
        "                    else:\n",
        "                        self.model.activations_[name] = input[0][:, num_tokens - 1].detach()\n",
        "\n",
        "            return hook\n",
        "\n",
        "        # Register hooks\n",
        "        self.model.transformer.h[0].ln_1.register_forward_hook(get_activation(\"input_embedding\"))\n",
        "\n",
        "        for i in range(self.model.config.n_layer):\n",
        "            if i != 0:\n",
        "                self.model.transformer.h[i].ln_1.register_forward_hook(get_activation(\"layer_residual_\" + str(i - 1)))\n",
        "            self.model.transformer.h[i].ln_2.register_forward_hook(get_activation(\"intermediate_residual_\" + str(i)))\n",
        "\n",
        "            self.model.transformer.h[i].attn.register_forward_hook(get_activation(\"attn_\" + str(i)))\n",
        "            self.model.transformer.h[i].mlp.register_forward_hook(get_activation(\"mlp_\" + str(i)))\n",
        "            self.model.transformer.h[i].mlp.c_proj.register_forward_hook(get_activation(\"m_coef_\" + str(i)))\n",
        "\n",
        "        self.model.transformer.ln_f.register_forward_hook(get_activation(\"layer_residual_\" + str(final_layer)))\n",
        "\n",
        "    def forward(self, text):\n",
        "        encoded_input = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        # Forward pass to trigger hooks\n",
        "        with torch.no_grad():\n",
        "            self.model(**encoded_input)\n",
        "\n",
        "        # Return activations\n",
        "        return self.model.activations_\n",
        "\n",
        "    def get_resid_predictions(self, sentence):\n",
        "        \"\"\"\n",
        "        This function computes the intermediate predictions at different layers of GPT-2\n",
        "        using activations from residual layers and intermediate layers.\n",
        "        \"\"\"\n",
        "        layer_residual_preds = []\n",
        "        intermed_residual_preds = []\n",
        "\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens.to(self.device)\n",
        "\n",
        "        # Output with hidden states\n",
        "        output = self.model(**tokens, output_hidden_states=True)\n",
        "\n",
        "        for layer in self.model.activations_.keys():\n",
        "            if \"layer_residual\" in layer or \"intermediate_residual\" in layer:\n",
        "                normed = self.model.transformer.ln_f(self.model.activations_[layer])\n",
        "\n",
        "                logits = torch.matmul(self.model.lm_head.weight, normed.T)\n",
        "\n",
        "                probs = F.softmax(logits.T[0], dim=-1)\n",
        "\n",
        "                probs = torch.reshape(probs, (-1,)).detach().cpu().numpy()\n",
        "\n",
        "                assert np.abs(np.sum(probs) - 1) <= 0.01, str(np.abs(np.sum(probs) - 1)) + layer\n",
        "\n",
        "                probs_ = []\n",
        "                for index, prob in enumerate(probs):\n",
        "                    probs_.append((index, prob))\n",
        "\n",
        "                # Get top-k predictions\n",
        "                top_k = sorted(probs_, key=lambda x: x[1], reverse=True)[:self.TOP_K]\n",
        "                top_k = [(t[1].item(), self.tokenizer.decode(t[0])) for t in top_k]\n",
        "\n",
        "            if \"layer_residual\" in layer:\n",
        "                layer_residual_preds.append(top_k)\n",
        "            elif \"intermediate_residual\" in layer:\n",
        "                intermed_residual_preds.append(top_k)\n",
        "\n",
        "        return layer_residual_preds, intermed_residual_preds\n",
        "\n",
        "    def display_predictions(self, sentence):\n",
        "        layer_residual_preds, intermed_residual_preds = self.get_resid_predictions(sentence)\n",
        "\n",
        "        print(f\"Predictions for: {sentence}\\n\")\n",
        "\n",
        "        # Display layer residual predictions\n",
        "        print(\"Layer Residual Predictions:\")\n",
        "        for i, preds in enumerate(layer_residual_preds):\n",
        "            print(f\"Layer {i}: {preds}\")\n",
        "\n",
        "        # Display intermediate residual predictions\n",
        "        print(\"\\nIntermediate Residual Predictions:\")\n",
        "        for i, preds in enumerate(intermed_residual_preds):\n",
        "            print(f\"Layer {i}: {preds}\")\n",
        "\n",
        "# Example usage\n",
        "gpt2_with_hooks = GPT2WithHooks()\n",
        "\n",
        "# Run some text through the model to collect activations\n",
        "sentence = \"\"\"Q: What is the capital of France?\n",
        "A: Paris\n",
        "Q: What is the capital of Poland?\n",
        "A:\"\"\"\n",
        "gpt2_with_hooks.display_predictions(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aOu-EnZCeQC",
        "outputId": "ba014d04-c09c-4b16-81fc-d6ba254f1b20"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for: Q: What is the capital of France?\n",
            "A: Paris\n",
            "Q: What is the capital of Poland?\n",
            "A:\n",
            "\n",
            "Layer Residual Predictions:\n",
            "Layer 0: [(0.02316313423216343, ' ('), (0.018392857164144516, ' ['), (0.012803674675524235, ' The'), (0.00920428428798914, ':'), (0.008853655308485031, ',')]\n",
            "Layer 1: [(0.019568050280213356, ' A'), (0.01659923605620861, ' The'), (0.014010702259838581, ' ('), (0.010149895213544369, ' ['), (0.007642451208084822, ' Is')]\n",
            "Layer 2: [(0.026887251064181328, ' A'), (0.018573066219687462, ' ['), (0.017433306202292442, ' ('), (0.013812464661896229, ' The'), (0.011615300551056862, ' At')]\n",
            "Layer 3: [(0.03929824382066727, ' A'), (0.022901220247149467, ' ['), (0.020305326208472252, ' ('), (0.018195513635873795, ' Act'), (0.017830180004239082, ' At')]\n",
            "Layer 4: [(0.038885049521923065, ' A'), (0.024648193269968033, ' ['), (0.023253243416547775, ' At'), (0.014608520083129406, ' Q'), (0.011924289166927338, ' (')]\n",
            "Layer 5: [(0.035387828946113586, ' A'), (0.033396828919649124, ' M'), (0.029982641339302063, ' No'), (0.014039270579814911, ' At'), (0.010575378313660622, ' The')]\n",
            "Layer 6: [(0.0901661291718483, ' No'), (0.08709050714969635, ' M'), (0.04744250327348709, ' A'), (0.02600138820707798, ' The'), (0.023228120058774948, ' C')]\n",
            "Layer 7: [(0.10190896689891815, ' C'), (0.056005530059337616, ' A'), (0.04421888291835785, ' No'), (0.03406987339258194, ' The'), (0.0249245073646307, ' M')]\n",
            "Layer 8: [(0.06418519467115402, ' A'), (0.050020862370729446, ' The'), (0.04732087627053261, ' C'), (0.04097878187894821, ' P'), (0.03743000328540802, ' H')]\n",
            "Layer 9: [(0.046277809888124466, ' A'), (0.041454847902059555, ' C'), (0.034639887511730194, ' No'), (0.030710438266396523, ' nil'), (0.029617032036185265, ' The')]\n",
            "Layer 10: [(0.06454506516456604, ' A'), (0.035239167511463165, ' The'), (0.033158134669065475, ' G'), (0.0314478725194931, ' C'), (0.024261601269245148, ' N')]\n",
            "Layer 11: [(0.0630597472190857, ' A'), (0.05588334798812866, ' C'), (0.05227632448077202, ' N'), (0.045361969619989395, ' None'), (0.04448726028203964, ' P')]\n",
            "Layer 12: [(0.08686946332454681, ' Unknown'), (0.08334776759147644, ' None'), (0.07776079326868057, ' C'), (0.05151363089680672, ' G'), (0.051125865429639816, ' A')]\n",
            "Layer 13: [(0.17199388146400452, ' C'), (0.08765115588903427, ' P'), (0.08286543935537338, ' N'), (0.0629202201962471, ' G'), (0.04469427838921547, ' B')]\n",
            "Layer 14: [(0.16741488873958588, ' St'), (0.0944088026881218, ' N'), (0.08308672904968262, ' G'), (0.08226499706506729, ' P'), (0.07120406627655029, ' Poland')]\n",
            "Layer 15: [(0.8781789541244507, ' Poland'), (0.02926219440996647, ' P'), (0.01606065034866333, ' St'), (0.01333632506430149, ' Pol'), (0.0060815694741904736, ' Warsaw')]\n",
            "Layer 16: [(0.9498633742332458, ' Poland'), (0.04541594535112381, ' Warsaw'), (0.004187267739325762, ' Polish'), (0.00020395935280248523, ' Poles'), (0.00012969886302016675, ' Budapest')]\n",
            "Layer 17: [(0.9197242259979248, ' Poland'), (0.07323256880044937, ' Warsaw'), (0.006632565520703793, ' Polish'), (0.00015307457942981273, ' Poles'), (9.344794671051204e-05, ' Budapest')]\n",
            "Layer 18: [(0.7380682826042175, ' Poland'), (0.2567846477031708, ' Warsaw'), (0.004197096452116966, ' Polish'), (0.0004322550958022475, ' Prague'), (0.0003290656895842403, ' Budapest')]\n",
            "Layer 19: [(0.8590308427810669, ' Warsaw'), (0.1380864679813385, ' Poland'), (0.0018533550901338458, ' Polish'), (0.0005096498061902821, ' Budapest'), (0.0003787627210840583, ' Prague')]\n",
            "Layer 20: [(0.9741461277008057, ' Warsaw'), (0.022860178723931313, ' Poland'), (0.001456715865060687, ' Prague'), (0.0007640245603397489, ' Budapest'), (0.0005554183735512197, ' Polish')]\n",
            "Layer 21: [(0.972966194152832, ' Warsaw'), (0.024995069950819016, ' Poland'), (0.0009886166080832481, ' Polish'), (0.0008147679618559778, ' Prague'), (0.00011722519411705434, ' Budapest')]\n",
            "Layer 22: [(0.9295430183410645, ' Warsaw'), (0.011738058179616928, ' Poland'), (0.009622957557439804, ' Prague'), (0.0048182448372244835, ' Budapest'), (0.0028199509251862764, ' K')]\n",
            "Layer 23: [(0.6972269415855408, ' Warsaw'), (0.0385914146900177, ' W'), (0.02709244191646576, ' K'), (0.014838638715445995, ' Br'), (0.014428702183067799, ' Po')]\n",
            "\n",
            "Intermediate Residual Predictions:\n",
            "Layer 0: [(0.9971626400947571, ':'), (0.0007807990768924356, ' conclud'), (0.0006193427834659815, ' enthusi'), (0.0004807538352906704, ' unden'), (0.00038159804535098374, ':]')]\n",
            "Layer 1: [(0.04497193172574043, ' ['), (0.016419339925050735, ' ('), (0.012967163696885109, ':'), (0.011227770708501339, ' A'), (0.010677666403353214, '\\n')]\n",
            "Layer 2: [(0.028469253331422806, ' A'), (0.01622842438519001, ' ['), (0.013808675110340118, ' The'), (0.012579196132719517, ' ('), (0.010162663646042347, ' Is')]\n",
            "Layer 3: [(0.02787005342543125, ' A'), (0.023719940334558487, ' ['), (0.012688802555203438, ' ('), (0.010959750041365623, ' The'), (0.01060269121080637, ' At')]\n",
            "Layer 4: [(0.03155406191945076, ' A'), (0.02442050725221634, ' ['), (0.017071956768631935, ' At'), (0.015813617035746574, ' ('), (0.009599128738045692, ' Act')]\n",
            "Layer 5: [(0.04547584429383278, ' A'), (0.027036890387535095, ' M'), (0.021933209151029587, ' Q'), (0.015308571048080921, ' At'), (0.014148605987429619, ' No')]\n",
            "Layer 6: [(0.10951510071754456, ' M'), (0.05407680943608284, ' No'), (0.040290746837854385, ' A'), (0.01651597023010254, ' Qu'), (0.016466476023197174, ' C')]\n",
            "Layer 7: [(0.14267240464687347, ' No'), (0.058490313589572906, ' C'), (0.05779298022389412, ' A'), (0.05278509110212326, ' M'), (0.01978517323732376, ' The')]\n",
            "Layer 8: [(0.1300898790359497, ' C'), (0.061887193471193314, ' A'), (0.04116178676486015, ' H'), (0.03155897557735443, ' P'), (0.03048756532371044, ' No')]\n",
            "Layer 9: [(0.0629938393831253, ' C'), (0.05807804316282272, ' H'), (0.056733421981334686, ' A'), (0.03667760267853737, ' No'), (0.03570182994008064, ' P')]\n",
            "Layer 10: [(0.04200495034456253, ' A'), (0.03704831749200821, ' Panama'), (0.03365152329206467, ' Unknown'), (0.02564295008778572, ' Pol'), (0.020910652354359627, ' The')]\n",
            "Layer 11: [(0.07194045186042786, ' A'), (0.034678392112255096, ' C'), (0.03243986889719963, ' Me'), (0.03087128885090351, ' G'), (0.028388667851686478, ' N')]\n",
            "Layer 12: [(0.08550843596458435, ' A'), (0.05780545622110367, ' C'), (0.054272543638944626, ' Ce'), (0.049171216785907745, ' G'), (0.04187167435884476, ' Unknown')]\n",
            "Layer 13: [(0.09578847885131836, ' Unknown'), (0.09140758961439133, ' C'), (0.07141861319541931, ' N'), (0.05503309518098831, ' A'), (0.04917553439736366, ' None')]\n",
            "Layer 14: [(0.13012105226516724, ' Poland'), (0.08049268275499344, ' P'), (0.07733916491270065, ' St'), (0.05919019877910614, ' G'), (0.046140871942043304, ' N')]\n",
            "Layer 15: [(0.8688827753067017, ' Poland'), (0.03722745180130005, ' P'), (0.012896629981696606, ' Pol'), (0.012229500338435173, ' St'), (0.00821639597415924, ' N')]\n",
            "Layer 16: [(0.9742953777313232, ' Poland'), (0.018254853785037994, ' Warsaw'), (0.00694187032058835, ' Polish'), (0.0002911186602432281, ' Poles'), (4.145610364503227e-05, ' Pol')]\n",
            "Layer 17: [(0.9488347172737122, ' Poland'), (0.04454698786139488, ' Warsaw'), (0.006214253604412079, ' Polish'), (0.0002427900180919096, ' Poles'), (5.083315045339987e-05, ' Budapest')]\n",
            "Layer 18: [(0.8832836747169495, ' Poland'), (0.11032620072364807, ' Warsaw'), (0.005808670073747635, ' Polish'), (0.00016911601414903998, ' Budapest'), (0.00014172385272104293, ' Prague')]\n",
            "Layer 19: [(0.7108008861541748, ' Poland'), (0.2835365831851959, ' Warsaw'), (0.005402250215411186, ' Polish'), (9.732529724715278e-05, ' Budapest'), (7.10867898305878e-05, ' Prague')]\n",
            "Layer 20: [(0.8439361453056335, ' Warsaw'), (0.15348376333713531, ' Poland'), (0.0020197448320686817, ' Polish'), (0.0002613672986626625, ' Budapest'), (0.00021726462000515312, ' Prague')]\n",
            "Layer 21: [(0.9500479698181152, ' Warsaw'), (0.04745896905660629, ' Poland'), (0.001989352982491255, ' Polish'), (0.0003521537291817367, ' Prague'), (0.00013087685510981828, ' Budapest')]\n",
            "Layer 22: [(0.9781054854393005, ' Warsaw'), (0.015999743714928627, ' Poland'), (0.0035531511530280113, ' Prague'), (0.0007155115017667413, ' Budapest'), (0.0006601338973268867, ' Polish')]\n",
            "Layer 23: [(0.8996348977088928, ' Warsaw'), (0.013082709163427353, ' Poland'), (0.012856709770858288, ' Prague'), (0.0067656091414391994, ' Budapest'), (0.0037235186900943518, ' W')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What exactly happens between Intermediate Residual Predictions and Layer Residual Predictions"
      ],
      "metadata": {
        "id": "SC2RLcJfDynK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
        "import inspect\n",
        "print(inspect.getsource(GPT2Block.forward))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efMj4Xvx3chT",
        "outputId": "323e8985-11c9-46ae-f2d0-c16e3aada148"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def forward(\n",
            "        self,\n",
            "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
            "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
            "        attention_mask: Optional[torch.FloatTensor] = None,\n",
            "        head_mask: Optional[torch.FloatTensor] = None,\n",
            "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
            "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
            "        use_cache: Optional[bool] = False,\n",
            "        output_attentions: Optional[bool] = False,\n",
            "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
            "        residual = hidden_states\n",
            "        hidden_states = self.ln_1(hidden_states)\n",
            "        attn_outputs = self.attn(\n",
            "            hidden_states,\n",
            "            layer_past=layer_past,\n",
            "            attention_mask=attention_mask,\n",
            "            head_mask=head_mask,\n",
            "            use_cache=use_cache,\n",
            "            output_attentions=output_attentions,\n",
            "        )\n",
            "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
            "        outputs = attn_outputs[1:]\n",
            "        # residual connection\n",
            "        hidden_states = attn_output + residual\n",
            "\n",
            "        if encoder_hidden_states is not None:\n",
            "            # add one self-attention block for cross-attention\n",
            "            if not hasattr(self, \"crossattention\"):\n",
            "                raise ValueError(\n",
            "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
            "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
            "                )\n",
            "            residual = hidden_states\n",
            "            hidden_states = self.ln_cross_attn(hidden_states)\n",
            "            cross_attn_outputs = self.crossattention(\n",
            "                hidden_states,\n",
            "                attention_mask=attention_mask,\n",
            "                head_mask=head_mask,\n",
            "                encoder_hidden_states=encoder_hidden_states,\n",
            "                encoder_attention_mask=encoder_attention_mask,\n",
            "                output_attentions=output_attentions,\n",
            "            )\n",
            "            attn_output = cross_attn_outputs[0]\n",
            "            # residual connection\n",
            "            hidden_states = residual + attn_output\n",
            "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
            "\n",
            "        residual = hidden_states\n",
            "        hidden_states = self.ln_2(hidden_states)\n",
            "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
            "        # residual connection\n",
            "        hidden_states = residual + feed_forward_hidden_states\n",
            "\n",
            "        if use_cache:\n",
            "            outputs = (hidden_states,) + outputs\n",
            "        else:\n",
            "            outputs = (hidden_states,) + outputs[1:]\n",
            "\n",
            "        return outputs  # hidden_states, present, (attentions, cross_attentions)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}